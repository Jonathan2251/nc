<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>NPU compiler &mdash; Neural Processor Unit Compiler</title>
    
    <link rel="stylesheet" href="_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Neural Processor Unit Compiler" href="index.html" />
    <link rel="next" title="ONNX" href="onnx.html" />
    <link rel="prev" title="Deep learning compiler" href="dlcompiler.html" /> 
  </head>
  <body role="document">
      <div class="header" role="banner"><h1 class="heading"><a href="index.html">
          <span>Neural Processor Unit Compiler</span></a></h1>
        <h2 class="heading"><span>NPU compiler</span></h2>
      </div>
      <div class="topnav" role="navigation" aria-label="top navigation">
      
        <p>
        «&#160;&#160;<a href="dlcompiler.html">Deep learning compiler</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="onnx.html">ONNX</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="npu-compiler">
<span id="sec-npu"></span><h1>NPU compiler<a class="headerlink" href="#npu-compiler" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#abstract" id="id10">Abstract</a></li>
<li><a class="reference internal" href="#mlir-and-iree" id="id11">MLIR and IREE</a></li>
<li><a class="reference internal" href="#tensorflow" id="id12">Tensorflow</a></li>
<li><a class="reference internal" href="#mlir-to-onnx" id="id13">mlir to onnx</a></li>
<li><a class="reference internal" href="#llvm-ir-for-npu-compiler" id="id14">llvm IR for NPU compiler</a></li>
<li><a class="reference internal" href="#open-source-project" id="id15">Open source project</a></li>
</ul>
</div>
<div class="section" id="abstract">
<h2><a class="toc-backref" href="#id10">Abstract</a><a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h2>
<p>Tensorflow support unknown shape <a class="footnote-reference" href="#tfunknownshape" id="id1">[2]</a>.
Though our npu support kernel call where kernel call is a set of
commands to npu to deal shape at run time, it is unefficiency.
As I remember mlit supports binding shape for unknown at compile-time
but not always work.
Lukily, we can customilze by redefining model to binding shape staticlly [20200412].</p>
</div>
<div class="section" id="mlir-and-iree">
<h2><a class="toc-backref" href="#id11">MLIR and IREE</a><a class="headerlink" href="#mlir-and-iree" title="Permalink to this headline">¶</a></h2>
<p>IREE (Intermediate Representation Execution Environment, pronounced as &#8220;eerie&#8221;)
is an MLIR-based end-to-end compiler that lowers ML models to a unified IR
optimized for real-time mobile/edge inference against heterogeneous hardware
accelerators. IREE also provides flexible deployment solutions for the compiled
ML models <a class="footnote-reference" href="#iree" id="id2">[1]</a> as the following figure.</p>
<div class="figure align-center" id="iree-f">
<a class="reference internal image-reference" href="_images/IREE-Architecture.png"><img alt="_images/IREE-Architecture.png" src="_images/IREE-Architecture.png" style="width: 1609.0px; height: 877.0px;" /></a>
</div>
<ul class="simple">
<li>HAL IR: Vulkan-like allocation and execution model encoding -&gt; on-line first-time compilation and save in cache. Executable compilation via architecture specific backend compiler plugins.</li>
<li>VM IR: Dynamic module linkage definitions (imports, exports, globals, etc) <a class="footnote-reference" href="#vm-ir-dml" id="id3">[3]</a>.</li>
</ul>
<p>The purpose of mlir is:</p>
<ul class="simple">
<li>Connect cpu with mlir-to-llvm-ir.</li>
</ul>
<p>The purpose of iree is:</p>
<ul class="simple">
<li>Connect gpu with iree-to-spirv.</li>
</ul>
<p>Both purpose of mlir and iree is:</p>
<ul class="simple">
<li>Reduce bug and problem between heterogeneous hardware accelerators <a class="footnote-reference" href="#mlir-iree-purpose" id="id4">[4]</a>.</li>
</ul>
</div>
<div class="section" id="tensorflow">
<h2><a class="toc-backref" href="#id12">Tensorflow</a><a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h2>
<p>The mechansim of Mlir and iree applied on tensorflow as the figure above section
is not fitted for off-line edge npu that stand alone without server-connection
for tunning weight of face detection&#8217;s purpose.
It is designed for on-line server-connected npu.
The gpu of supporting spirv is best candidate until this date 2020/5/12.</p>
<p>At beginning, tensorflow rely on api without fixed format such as ONNX <a class="footnote-reference" href="#onnx-fmt" id="id5">[5]</a>.
As a result ONNX emerged and adopted for most of npu in their private backend
compiler. Google does not like to hire onnx as the format for npu backend compiler
onnx-mlir project <a class="footnote-reference" href="#onnx-mlir" id="id6">[6]</a> which convert onnx to mlir dialect is sponsored
by Google I guess <a class="footnote-reference" href="#onnx-mlir-sponsor" id="id7">[7]</a> for encourging new npu compiler
development hiring mlir as their compiler input (convert onnx to mlir then
handling mlir input).</p>
<p>With mlir and iree appear on tensorflow as a series of fixed formats in
tensorflow as section above. The hardware vendors for cloud server AI machine
with heterogeneous hardware accelerators will run tensorflow system
by supporting mlir/iree input format in their compilers more and more.
So, it is unavoidable that tensorflow system&#8217;s npu vendors have to support
mlir/iree input format beyond onnx. Or open source software or vendor software
appear to do transfer from mlir/iree to onnx. (python in tensorflow api allow
unknown type and shape size, so it cannot transer python api to onnx fully).</p>
<p>If lucky, google may hire onnx. Because onnx format is older than mlir
in history. In addition in aspect of format, mlir has mult-level mult-dialect and
more complicate while onnx is easy and better to understand (P.S. I don&#8217;t dig
into mlir yet).
Many AI models has supported onnx file format. For some AI model&#8217;s formats that
run on tensorflow without supporting onnx, aplly tensorflow-onnx open
source project <a class="footnote-reference" href="#tf-onnx" id="id8">[8]</a> can convert tensorflow to onnx partly.</p>
<p>Onnx alliance may release some programs for transfering mlir to onnx for fighting
agiant mlir-iree growing in npu compiler but not at this moment.</p>
<p>For off-line edge npu that stand alone without server-connection
for tunning weight of face detection&#8217;s purpose, supprting mlir-iree compiler
may not necessary.</p>
</div>
<div class="section" id="mlir-to-onnx">
<h2><a class="toc-backref" href="#id13">mlir to onnx</a><a class="headerlink" href="#mlir-to-onnx" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.tensorflow.org/mlir">https://www.tensorflow.org/mlir</a></p>
<p><a class="reference external" href="https://mlir.llvm.org/talks/">https://mlir.llvm.org/talks/</a></p>
<p><a class="reference external" href="https://llvm.org/devmtg/2019-04/talks.html#Tutorial_1">https://llvm.org/devmtg/2019-04/talks.html#Tutorial_1</a></p>
<ul class="simple">
<li>3 ppt in llvm tutorials</li>
</ul>
<p><a class="reference external" href="https://llvm.org/devmtg/2019-04/slides/Tutorial-AminiVasilacheZinenko-MLIR.pdf">https://llvm.org/devmtg/2019-04/slides/Tutorial-AminiVasilacheZinenko-MLIR.pdf</a></p>
<p>build mlir: <a class="reference external" href="https://mlir.llvm.org/getting_started/">https://mlir.llvm.org/getting_started/</a></p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">~/llvm/1/llvm-project/build$ cmake -G Ninja ../llvm \</span>
<span class="gp">&gt;</span>    -DLLVM_ENABLE_PROJECTS<span class="o">=</span>mlir <span class="se">\</span>
<span class="gp">&gt;</span>    -DLLVM_BUILD_EXAMPLES<span class="o">=</span>ON <span class="se">\</span>
<span class="gp">&gt;</span>    -DLLVM_TARGETS_TO_BUILD<span class="o">=</span><span class="s2">&quot;X86;NVPTX;AMDGPU&quot;</span> <span class="se">\</span>
<span class="gp">&gt;</span>    -DCMAKE_BUILD_TYPE<span class="o">=</span>Release <span class="se">\</span>
<span class="gp">&gt;</span>    -DLLVM_ENABLE_ASSERTIONS<span class="o">=</span>ON

<span class="go">~/llvm/1/llvm-project/build$ cmake --build . --target check-mlir</span>
<span class="go">[200/1919] Generating VCSRevision.h</span>
<span class="go">-- Found Git: /usr/bin/git (found version &quot;2.17.1&quot;)</span>
<span class="go">[1604/1919] Building CXX object tools/mlir/tools/mlir-linalg-ods-gen/CMakeFiles/mlir-linalg-ods-gen.dir/mlir-linalg-ods-gen.cpp.o</span>
<span class="go">/home/cschen/llvm/1/llvm-project/mlir/tools/mlir-linalg-ods-gen/mlir-linalg-ods-gen.cpp:935:6: warning: ‘bool {anonymous}::Expression::operator==(const {anonymous}::Expression&amp;) const’ defined but not used [-Wunused-function]</span>
<span class="go"> bool Expression::operator==(const Expression &amp;e) const {</span>
<span class="go">      ^~~~~~~~~~</span>
<span class="go">[1918/1919] Running the MLIR regression tests</span>

<span class="go">Testing Time: 9.88s</span>
<span class="go">  Unsupported Tests:  16</span>
<span class="go">  Expected Passes  : 465</span>
</pre></div>
</div>
<p>run: <a class="reference external" href="https://mlir.llvm.org/docs/Tutorials/Toy/">https://mlir.llvm.org/docs/Tutorials/Toy/</a></p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/toyc-ch1 ast.toy -emit=ast</span>
<span class="go">...</span>
<span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/toyc-ch1 ast.toy -emit=ast 2&gt;&amp;1 | ~/llvm/1/llvm-project/build/bin/FileCheck ast.toy</span>
<span class="go">~/llvm/1/llvm-project/mlir/test/Examples/Toy/Ch1$ ~/llvm/1/llvm-project/build/bin/llvm-lit ast.toy</span>
<span class="go">-- Testing: 1 tests, 1 workers --</span>
<span class="go">PASS: MLIR :: Examples/Toy/Ch1/ast.toy (1 of 1)</span>

<span class="go">Testing Time: 0.11s</span>
<span class="go">  Expected Passes: 1</span>
</pre></div>
</div>
<p>The result I run is based on git commit 455ccde1377b3ec32d321eb7c38808fecdf230a8 Date:   Sun May 17 21:00:09 2020 -0400</p>
</div>
<div class="section" id="llvm-ir-for-npu-compiler">
<h2><a class="toc-backref" href="#id14">llvm IR for NPU compiler</a><a class="headerlink" href="#llvm-ir-for-npu-compiler" title="Permalink to this headline">¶</a></h2>
<p>Though npu has no general purpose registers GPR, it is possible to apply llvm ir for
npu to do codegen by llvm as follows,</p>
<div class="figure align-center" id="id9">
<span id="conv"></span><a class="reference internal image-reference" href="_images/conv_onnx.png"><img alt="_images/conv_onnx.png" src="_images/conv_onnx.png" style="width: 702.0px; height: 450.0px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Conv operation in onnx file</span></p>
</div>
<div class="highlight-llvm"><div class="highlight"><pre><span></span><span class="vg">@x1</span> <span class="p">=</span> <span class="k">global</span> <span class="p">[</span><span class="m">1</span> <span class="k">x</span> <span class="p">[</span><span class="m">3</span> <span class="k">x</span> <span class="p">[</span><span class="m">120</span> <span class="k">x</span> <span class="p">[</span><span class="m">120</span> <span class="k">x</span> <span class="kt">float</span><span class="p">]]]],</span> <span class="k">align</span> <span class="m">4</span>
<span class="vg">@w1</span> <span class="p">=</span> <span class="k">global</span> <span class="p">[</span><span class="m">64</span> <span class="k">x</span> <span class="p">[</span><span class="m">3</span> <span class="k">x</span> <span class="p">[</span><span class="m">7</span> <span class="k">x</span> <span class="p">[</span><span class="m">7</span> <span class="k">x</span> <span class="kt">float</span><span class="p">]]]],</span> <span class="k">align</span> <span class="m">4</span>
<span class="vg">@conv</span> <span class="p">=</span> <span class="vg">@llvm.npu1.conv</span> <span class="kt">float</span><span class="p">*</span> <span class="vg">@x</span><span class="p">,</span> <span class="kt">float</span><span class="p">*</span> <span class="vg">@weight</span><span class="p">,</span> <span class="p">...</span>
</pre></div>
</div>
<p>Conclusion:</p>
<blockquote>
<div><ol class="arabic simple">
<li>No GPRs in NPU but can get advantage of code-gen by llvm-tblgen tool.</li>
<li>The vector size of llvm is power of 2 (1, 2, 4, 8, ...). But it can be achieved by modifying llvm kernel source data type.</li>
</ol>
<p>ref. code/llvm-ex1.c</p>
</div></blockquote>
<p>reference:</p>
<blockquote>
<div><ul class="simple">
<li>5.2.2  Code Generation based on Low-Level IR.The low-level IR adopted by most DL compilers canbe eventually lowered to LLVM IR, and benefits from LLVM’s mature optimizer and code generator.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="open-source-project">
<h2><a class="toc-backref" href="#id15">Open source project</a><a class="headerlink" href="#open-source-project" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>onnx to mlir dialect: <a class="reference external" href="https://github.com/onnx/onnx-mlir">https://github.com/onnx/onnx-mlir</a></li>
<li>tensorflow to onnx: <a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a></li>
<li>onnx to tensorflow: <a class="reference external" href="https://github.com/onnx/onnx-tensorflow">https://github.com/onnx/onnx-tensorflow</a></li>
</ul>
<table class="docutils footnote" frame="void" id="iree" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td><a class="reference external" href="https://github.com/google/iree">https://github.com/google/iree</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="tfunknownshape" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[2]</a></td><td><a class="reference external" href="https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/">https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="vm-ir-dml" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td>Page 15 of <a class="reference external" href="https://docs.google.com/presentation/d/1RCQ4ZPQFK9cVgu3IH1e5xbrBcqy7d_cEZ578j84OvYI/edit#slide=id.g6e31674683_0_23101">https://docs.google.com/presentation/d/1RCQ4ZPQFK9cVgu3IH1e5xbrBcqy7d_cEZ578j84OvYI/edit#slide=id.g6e31674683_0_23101</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="mlir-iree-purpose" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[4]</a></td><td><a class="reference external" href="https://kknews.cc/zh-tw/tech/klkombr.html">https://kknews.cc/zh-tw/tech/klkombr.html</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="onnx-fmt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[5]</a></td><td>Actually onnx format based on IO api with protobuffer. It has real binary format but may change from version to version. Tensorflow api has no real binary format.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="onnx-mlir" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[6]</a></td><td><a class="reference external" href="https://github.com/onnx/onnx-mlir">https://github.com/onnx/onnx-mlir</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="onnx-mlir-sponsor" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[7]</a></td><td><a class="reference external" href="https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/2FT4sD8kqTY">https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/2FT4sD8kqTY</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="tf-onnx" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[8]</a></td><td><a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a></td></tr>
</tbody>
</table>
</div>
</div>


      </div>
      <div class="bottomnav" role="navigation" aria-label="bottom navigation">
      
        <p>
        «&#160;&#160;<a href="dlcompiler.html">Deep learning compiler</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="onnx.html">ONNX</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2016, Chen Chung-Shu.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.4.4.
    </div>
  </body>
</html>